{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Homework2V0FirstVersion.ipynb","provenance":[{"file_id":"15uimhIG5u653qfcwqqS-Mx99ZpSpiAPx","timestamp":1607611732067},{"file_id":"1s44SCmeHj3nnNeCsqmt5BQ8hs_eQfX3Z","timestamp":1607505957854},{"file_id":"1MdlFUgQgG0Y1gw9ypYZWRJ3Kz0t6raPf","timestamp":1607197204888}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"4pu-Eb9BC-ko"},"source":["from IPython.core.interactiveshell import InteractiveShell\n","InteractiveShell.ast_node_interactivity = \"all\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V_Dp8ntCDDIr"},"source":["import os\n","\n","import tensorflow as tf\n","import numpy as np\n","\n","# Set the seed for random operations. \n","SEED = 1234\n","tf.random.set_seed(SEED)  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NAE39GrHDHAv","executionInfo":{"status":"ok","timestamp":1607671819243,"user_tz":-60,"elapsed":22901,"user":{"displayName":"Nicole Bacchetta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjyFF9ioxzs8L3MRFO2ADm6SYb30TLkF1mRoC7V=s64","userId":"16559632982838470851"}},"outputId":"af5f85e1-dc43-4075-8ff6-d58b71642946"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UyS0LiHxDLYe"},"source":["cwd = os.getcwd()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GbsoXbZvDL7I"},"source":["!unzip '/content/drive/My Drive/Homeworks/Homework2/Datasets/Development_Dataset'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z6EJz8fwbWAr"},"source":["#Script that change Roseau images extension from png to jpg\n","from PIL import Image\n","import shutil\n","\n","plant_list = ['Haricot', 'Mais']\n","\n","for plant in plant_list:\n","  images_path = '/content/Development_Dataset/Training/Roseau/' + plant + '/Images/'\n","  file_names = os.listdir(images_path)\n","  \n","  for file_name in file_names:\n","    no_extension_name = file_name[:-4]\n","    image = Image.open(images_path + file_name)\n","    image.save(images_path + no_extension_name + '.jpg')\n","    os.remove(images_path + file_name)\n","\n","  images_path = '/content/Development_Dataset/Test_Dev/Roseau/' + plant + '/Images/'\n","  file_names = os.listdir(images_path)\n","  \n","  for file_name in file_names:\n","    no_extension_name = file_name[:-4]\n","    image = Image.open(images_path + file_name)\n","    image.save(images_path + no_extension_name + '.jpg')\n","    os.remove(images_path + file_name)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wCn9C-hpEfPj"},"source":["# ImageDataGenerator\n","# ------------------\n","\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","apply_data_augmentation = True\n","\n","# Create training ImageDataGenerator object\n","# We need two different generators for images and corresponding masks\n","if apply_data_augmentation:\n","    img_data_gen = ImageDataGenerator(rotation_range=0.3,\n","                                      width_shift_range=0.2,\n","                                      height_shift_range=0.2,\n","                                      zoom_range=0.1,\n","                                      horizontal_flip=True,\n","                                      vertical_flip=True,\n","                                      fill_mode='reflect')\n","    mask_data_gen = ImageDataGenerator(rotation_range=0.3,\n","                                       width_shift_range=0.2,\n","                                       height_shift_range=0.2,\n","                                       zoom_range=0.1,\n","                                       horizontal_flip=True,\n","                                       vertical_flip=True,\n","                                       fill_mode='reflect')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m8n5pYDzOlGG"},"source":["#Script from challenge starting_kit\n","def read_rgb_mask(img_path, out_shape):\n","    '''\n","    img_path: path to the mask file\n","    Returns the numpy array containing target values\n","    '''\n","\n","    mask_img = Image.open(img_path)\n","\n","    # the resizing was added in order to have input images of fixed size\n","    mask_img = mask_img.resize(out_shape, resample=Image.NEAREST)\n","    \n","\n","    mask_arr = np.array(mask_img)\n","\n","    new_mask_arr = np.zeros(mask_arr.shape[:2], dtype=mask_arr.dtype)\n","\n","    # Use RGB dictionary in 'RGBtoTarget.txt' to convert RGB to target\n","    new_mask_arr[np.where(np.all(mask_arr == [216, 124, 18], axis=-1))] = 0\n","    new_mask_arr[np.where(np.all(mask_arr == [255, 255, 255], axis=-1))] = 1\n","    new_mask_arr[np.where(np.all(mask_arr == [216, 67, 82], axis=-1))] = 2\n","\n","    return new_mask_arr\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YAIZ1dHnEhio"},"source":["#This is the CustomDataset class used to generate Test and Validation sets. Is inspired by the one seen during lab.\n","from PIL import Image\n","\n","class CustomDataset(tf.keras.utils.Sequence):\n","\n","  \"\"\"\n","    CustomDataset inheriting from tf.keras.utils.Sequence.\n","\n","    3 main methods:\n","      - __init__: save dataset params like directory, filenames..\n","      - __len__: return the total number of samples in the dataset\n","      - __getitem__: return a sample from the dataset\n","\n","    Note: \n","      - the custom dataset return a single sample from the dataset. Then, we use \n","        a tf.data.Dataset object to group samples into batches.\n","      - in this case we have a different structure of the dataset in memory. \n","        We have all the images in the same folder and the training and validation splits\n","        are defined in text files.\n","\n","  \"\"\"\n","\n","  def __init__(self, dataset_dir, which_subset, img_generator=None, mask_generator=None, \n","               preprocessing_function=None, out_shape=[256, 256]):\n","    \n","    images = os.listdir(os.path.join(dataset_dir, 'Images'))\n","\n","    #Shuffle the images order\n","    import random\n","    random.seed(SEED)\n","    images = os.listdir(os.path.join(dataset_dir, 'Images'))\n","    random.shuffle(images)\n","    #Split the whole set in training (80%) and validation (20%)\n","    splitted_images = np.array_split(images,[round(len(images)*0.8)])\n","    train_images = splitted_images[0]\n","    valid_images = splitted_images[1]\n","    if which_subset == 'training':\n","      lines = train_images\n","    elif which_subset == 'validation':\n","      lines = valid_images\n","\n","    lines = images\n","\n","    subset_filenames = []\n","    for line in lines:\n","      subset_filenames.append(line.strip('.jpg'))   \n","\n","    self.which_subset = which_subset\n","    self.dataset_dir = dataset_dir\n","    self.subset_filenames = subset_filenames\n","    self.img_generator = img_generator\n","    self.mask_generator = mask_generator\n","    self.preprocessing_function = preprocessing_function\n","    self.out_shape = out_shape\n","\n","  def __len__(self):\n","    return len(self.subset_filenames)\n","\n","  def __getitem__(self, index):\n","    # Read Image\n","    curr_filename = self.subset_filenames[index]\n","    img = Image.open(os.path.join(self.dataset_dir, 'Images', curr_filename + '.jpg'))\n","    mask_path = os.path.join(self.dataset_dir, 'Masks', curr_filename + '.png')\n","\n","    # Resize image and mask\n","    img = img.resize(self.out_shape)\n","    # Resizing mask is inside read_rgb_mask\n","\n","    img_arr = np.array(img)\n","    \n","    mask_arr = read_rgb_mask(mask_path, out_shape=self.out_shape)\n","\n","\n","    mask_arr = np.expand_dims(mask_arr, -1)\n","\n","    if self.which_subset == 'training':\n","      if self.img_generator is not None and self.mask_generator is not None:\n","        # Perform data augmentation\n","        # We can get a random transformation from the ImageDataGenerator using get_random_transform\n","        # and we can apply it to the image using apply_transform\n","        img_t = self.img_generator.get_random_transform(img_arr.shape, seed=SEED)\n","        mask_t = self.mask_generator.get_random_transform(mask_arr.shape, seed=SEED)\n","        img_arr = self.img_generator.apply_transform(img_arr, img_t)\n","        # ImageDataGenerator use bilinear interpolation for augmenting the images.\n","        # Thus, when applied to the masks it will output 'interpolated classes', which\n","        # is an unwanted behaviour. As a trick, we can transform each class mask \n","        # separately and then we can cast to integer values.\n","        # Finally, we merge the augmented binary masks to obtain the final segmentation mask.\n","        \n","        out_mask = np.zeros_like(mask_arr)\n","        for c in np.unique(mask_arr):\n","          if c > 0:\n","            curr_class_arr = np.float32(mask_arr == c)\n","            curr_class_arr = self.mask_generator.apply_transform(curr_class_arr, mask_t)\n","            # from [0, 1] to {0, 1}\n","            curr_class_arr = np.uint8(curr_class_arr)\n","            # recover original class\n","            curr_class_arr = curr_class_arr * c \n","            out_mask += curr_class_arr\n","    else:\n","      out_mask = mask_arr\n","    \n","    if self.preprocessing_function is not None:\n","        img_arr = self.preprocessing_function(img_arr)\n","\n","    return img_arr, np.float32(out_mask)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0nNQqFkmHfsu"},"source":["# Script to group all the image from team_list and plant_list in a new path, for both Training and Test_Dev\n","team_list = ['Bipbip', 'Weedelec', 'Roseau', 'Pead'] # 'Bipbip', 'Pead', 'Roseau', 'Weedelec'\n","plant_list = ['Haricot', 'Mais']\n","\n","groupped_images = os.path.join(cwd, 'Groupped_Images')\n","if not os.path.exists(groupped_images):\n","    os.makedirs(groupped_images)\n","\n","groupped_training = os.path.join(groupped_images, 'Training')\n","if not os.path.exists(groupped_training):\n","    os.makedirs(groupped_training)\n","    os.makedirs(os.path.join(groupped_training, 'Images'))\n","    os.makedirs(os.path.join(groupped_training, 'Masks'))\n","\n","groupped_test_dev = os.path.join(groupped_images, 'Test_Dev')\n","if not os.path.exists(groupped_test_dev):\n","    os.makedirs(groupped_test_dev)\n","    os.makedirs(os.path.join(groupped_test_dev, 'Images'))\n","\n","\n","for team in team_list:\n","  for plant in plant_list:\n","    source_dir = '/content/Development_Dataset/Training/' + team + '/' + plant + '/Images'  \n","    target_dir = '/content/Groupped_Images/Training/Images'\n","    \n","    file_names = os.listdir(source_dir)\n","    for file_name in file_names:\n","      shutil.copy(os.path.join(source_dir, file_name), target_dir)\n","      \n","    source_dir = '/content/Development_Dataset/Training/' + team + '/' + plant + '/Masks'  \n","    target_dir = '/content/Groupped_Images/Training/Masks'\n","    \n","    file_names = os.listdir(source_dir)\n","    \n","    for file_name in file_names:\n","      shutil.copy(os.path.join(source_dir, file_name), target_dir)\n","\n","\n","    source_dir = '/content/Development_Dataset/Test_Dev/' + team + '/' + plant + '/Images'  \n","    target_dir = '/content/Groupped_Images/Test_Dev/Images'\n","    \n","    file_names = os.listdir(source_dir)\n","    for file_name in file_names:\n","      shutil.copy(os.path.join(source_dir, file_name), target_dir)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wt0UqkqYG02Y"},"source":["#Create the training and validation generators\n","from tensorflow.keras.applications.vgg16 import preprocess_input \n","train_dataset_dir = os.path.join(cwd, 'Groupped_Images/Training')\n","img_h = 512\n","img_w = 512\n","\n","dataset = CustomDataset(train_dataset_dir, 'training', \n","                        img_generator=img_data_gen, mask_generator=mask_data_gen,\n","                        preprocessing_function=None, out_shape=[512, 512])\n","dataset_valid = CustomDataset(train_dataset_dir, 'validation', \n","                              preprocessing_function=None, out_shape=[512, 512])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Zgk-fC0wC6LN"},"source":["#Create Training and Validation datasets\n","batch_size = 32\n","\n","train_dataset = tf.data.Dataset.from_generator(lambda: dataset,\n","                                               output_types=(tf.float32, tf.float32),\n","                                               output_shapes=([img_h, img_w, 3], [img_h, img_w, 1]))\n","\n","train_dataset = train_dataset.batch(batch_size)\n","\n","train_dataset = train_dataset.repeat()\n","\n","valid_dataset = tf.data.Dataset.from_generator(lambda: dataset_valid,\n","                                               output_types=(tf.float32, tf.float32),\n","                                               output_shapes=([img_h, img_w, 3], [img_h, img_w, 1]))\n","valid_dataset = valid_dataset.batch(batch_size)\n","\n","valid_dataset = valid_dataset.repeat()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WNK46b45RdMD"},"source":["# Let's test data generator-> visualize image and correspondent mask \n","# -------------------------\n","import time\n","from matplotlib import cm\n","import matplotlib.pyplot as plt\n","\n","%matplotlib inline\n","\n","# Assign a color to each class\n","evenly_spaced_interval = np.linspace(0, 1, 20)\n","colors = [cm.rainbow(x) for x in evenly_spaced_interval]\n","\n","iterator = iter(train_dataset)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zNQ4hYZjcwF1"},"source":["\n","\n","# Assign a color to each class\n","colors_dict = {}\n","colors_dict[2] = [255, 0, 0]  # weed\n","colors_dict[1] = [255, 255, 255]  # crop\n","colors_dict[0] = [0, 0, 0]  # background \n","\n","\n","fig, ax = plt.subplots(1, 2)\n","augmented_img, target = next(iterator)\n","augmented_img = augmented_img[0]   # First element\n","augmented_img = augmented_img  # denormalize\n","target = np.array(target[0, ..., 0])   # First element (squeezing channel dimension)\n","\n","print(np.unique(target))\n","\n","target_img = np.zeros([target.shape[0], target.shape[1], 3])\n","\n","target_img[np.where(target == 0)] = [0, 0, 0]\n","for i in range(1, 3):\n","  target_img[np.where(target == i)] = colors_dict[i]\n","\n","ax[0].imshow(np.uint8(augmented_img))\n","ax[1].imshow(np.uint8(target_img))\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TjtvMKxqep32"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yVJFEbsqexL8"},"source":["#Is the first model that was used to experimenting the segmentation problem, same as the one used in Multiclass segmentation lab \n","vgg = tf.keras.applications.VGG16(weights='imagenet', include_top=False, input_shape=(img_h, img_w, 3))\n","vgg.summary()\n","\n","for layer in vgg.layers:\n","  layer.trainable = False\n","\n","def create_model(depth, start_f, num_classes):\n","\n","    model = tf.keras.Sequential()\n","    \n","    # Encoder\n","    # -------\n","    model.add(vgg)\n","    \n","    start_f = 256\n","        \n","    # Decoder\n","    # -------\n","    for i in range(depth):\n","        model.add(tf.keras.layers.UpSampling2D(2, interpolation='bilinear'))\n","        model.add(tf.keras.layers.Conv2D(filters=start_f,\n","                                         kernel_size=(3, 3),\n","                                         strides=(1, 1),\n","                                         padding='same'))\n","        model.add(tf.keras.layers.ReLU())\n","\n","        start_f = start_f // 2\n","\n","    # Prediction Layer\n","    # ----------------\n","    model.add(tf.keras.layers.Conv2D(filters=num_classes,\n","                                     kernel_size=(1, 1),\n","                                     strides=(1, 1),\n","                                     padding='same',\n","                                     activation='softmax'))\n","    \n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oe_cQrW7Gqgw"},"source":["#Alternative model, implement a custom UNet \n","from tensorflow.keras import layers\n","def get_model(img_size, num_classes):\n","    inputs = tf.keras.Input(shape=img_size + (3,))\n","\n","    ### [First half of the network: downsampling inputs] ###\n","\n","    # Entry block\n","    x = layers.Conv2D(32, 3, strides=2, padding=\"same\")(inputs)\n","    x = layers.BatchNormalization()(x)\n","    x = layers.Activation(\"relu\")(x)\n","\n","    previous_block_activation = x  # Set aside residual\n","\n","    # Blocks 1, 2, 3 are identical apart from the feature depth.\n","    for filters in [64, 128, 256]:\n","        x = layers.Activation(\"relu\")(x)\n","        x = layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n","        x = layers.BatchNormalization()(x)\n","\n","        x = layers.Activation(\"relu\")(x)\n","        x = layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n","        x = layers.BatchNormalization()(x)\n","\n","        x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n","\n","        # Project residual\n","        residual = layers.Conv2D(filters, 1, strides=2, padding=\"same\")(\n","            previous_block_activation\n","        )\n","        x = layers.add([x, residual])  # Add back residual\n","        previous_block_activation = x  # Set aside next residual\n","\n","    ### [Second half of the network: upsampling inputs] ###\n","\n","\n","    for filters in [256, 128, 64, 32]:\n","        x = layers.Activation(\"relu\")(x)\n","        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n","        x = layers.BatchNormalization()(x)\n","\n","        x = layers.Activation(\"relu\")(x)\n","        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n","        x = layers.BatchNormalization()(x)\n","\n","        x = layers.UpSampling2D(2)(x)\n","\n","        # Project residual\n","        residual = layers.UpSampling2D(2)(previous_block_activation)\n","        residual = layers.Conv2D(filters, 1, padding=\"same\")(residual)\n","        x = layers.add([x, residual])  # Add back residual\n","        previous_block_activation = x  # Set aside next residual\n","\n","    # Add a per-pixel classification layer\n","    outputs = layers.Conv2D(num_classes, 3, activation=\"softmax\", padding=\"same\")(x)\n","\n","    # Define the model\n","    model = tf.keras.Model(inputs, outputs)\n","    return model\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"usx0rwYiQvXb"},"source":["#This is the main UNet that was used for our experiments.\n","#It is composed of a VGG encoder, a 5 levels decoder with skip connections\n","from keras.layers import Conv2D, Conv2DTranspose, concatenate\n","from keras import Model\n","\n","from keras.applications.vgg16 import VGG16\n","img_h = 512\n","img_w = 512\n","base_pretrained_model = VGG16(input_shape =  (img_h,img_w, 3), include_top = False, weights = 'imagenet')\n","base_pretrained_model.summary()\n","inp = base_pretrained_model.input\n","output_vgg16_conv = base_pretrained_model.output\n","fine_tuning = True\n","if fine_tuning:\n","  for layer in base_pretrained_model.layers:\n","    if ('conv3' in layer.name or 'conv2' in layer.name):\n","      layer.trainable = True\n","    else:\n","      layer.trainable = False\n","else:\n","  for layer in base_pretrained_model.layers:\n","    layer.trainable = False\n","\n","\n","# output and start upsampling\n","conv_1  = Conv2D(512, (3,3), activation='relu', padding='same')(output_vgg16_conv)\n","up_conv = Conv2DTranspose(256, (3,3), strides=(2,2), activation='relu', padding='same')(conv_1)\n","\n","# first concatenation block\n","concat_1 = concatenate([base_pretrained_model.get_layer('block5_conv3').output, up_conv], axis=-1, name='concat_1')\n","conv_2 = Conv2D(512, (3,3), activation='relu', padding='same')(concat_1)\n","up_conv_2 = Conv2DTranspose(256, (3,3), strides=(2,2), activation='relu', padding='same')(conv_2)\n","\n","# second concatenation block\n","concat_2 = concatenate([up_conv_2, base_pretrained_model.get_layer('block4_conv3').output])\n","conv_3 = Conv2D(512, (3,3), activation='relu', padding='same')(concat_2)\n","up_conv_3 = Conv2DTranspose(128, (3,3), strides=(2,2), activation='relu', padding='same')(conv_3)\n","\n","# third concatenation block\n","concat_3 = concatenate([up_conv_3, base_pretrained_model.get_layer('block3_conv3').output])\n","conv_4 = Conv2D(256, (3,3), activation='relu', padding='same')(concat_3)\n","up_conv_4 = Conv2DTranspose(64, (3,3), strides=(2,2), activation='relu', padding='same')(conv_4)\n","\n","# fourth concatenation block\n","concat_4 = concatenate([up_conv_4, base_pretrained_model.get_layer('block2_conv2').output])\n","conv_5 = Conv2D(128, (3,3), activation='relu', padding='same')(concat_4)\n","up_conv_5 = Conv2DTranspose(32, (3,3), strides=(2,2), activation='relu', padding='same')(conv_5)\n","\n","# fifth concatenation block\n","concat_4 = concatenate([up_conv_5, base_pretrained_model.get_layer('block1_conv2').output])\n","conv_6 = Conv2D(128, (3,3), activation='sigmoid', padding='same')(concat_4)\n","\n","# Add a per-pixel classification layer\n","outputs = Conv2D(3, 3, activation=\"softmax\", padding=\"same\")(conv_6)\n","\n","finalModel = Model(inp, outputs = outputs)\n","finalModel.summary()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yBinoFp01RZ9"},"source":["# Free up RAM in case the model definition cells were run multiple times\n","tf.keras.backend.clear_session()\n","\n","# Build model\n","model = final_model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XbAGs6Vfhf1r"},"source":["# Optimization params\n","# -------------------\n","\n","# Loss\n","#In order to reduce the classes unbalance, a weighted custom loss is implemented.\n","def meanIoU_loss(y_true, y_pred, weights):\n","    y_pred = tf.expand_dims(tf.argmax(y_pred, -1), -1)\n","\n","    per_class_iou = []\n","\n","    for i in range(0,3): \n","      # Get prediction and target related to only a single class (i)\n","      class_pred = tf.cast(tf.where(y_pred == i, 1, 0), tf.float32)\n","      class_true = tf.cast(tf.where(y_true == i, 1, 0), tf.float32)\n","      intersection = tf.reduce_sum(class_true * class_pred)\n","      union = tf.reduce_sum(class_true) + tf.reduce_sum(class_pred) - intersection\n","    \n","      iou = (intersection + 1e-8) / (union + 1e-8)\n","      per_class_iou.append(iou)\n","\n","    return (per_class_iou[0] * weights[0] + per_class_iou[1] * weights[1] + per_class_iou[2] * weights[2])/(weights[0]+weights[1]+weights[2])\n","\n","#The weighted_loss compute the weighted mean IoU of each class and multiplies it for sparse categorical cross entropy\n","def weighted_loss(y_true, y_pred,  weights=[1,6,10]):\n","  return (1-meanIoU_loss(y_true,y_pred, weights))*tf.keras.losses.sparse_categorical_crossentropy(y_true,y_pred)\n","\n","loss = weighted_loss\n","\n","# learning rate\n","lr = 1e-3\n","optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n","# -------------------\n","\n","# Here we define the intersection over union for each class in the batch.\n","# Then we compute the final iou as the mean over classes\n","def meanIoU(y_true, y_pred):\n","    # get predicted class from softmax\n","    y_pred = tf.expand_dims(tf.argmax(y_pred, -1), -1)\n","\n","    per_class_iou = []\n","\n","    for i in range(1,3): # exclude the background class 0\n","      # Get prediction and target related to only a single class (i)\n","      class_pred = tf.cast(tf.where(y_pred == i, 1, 0), tf.float32)\n","      class_true = tf.cast(tf.where(y_true == i, 1, 0), tf.float32)\n","      intersection = tf.reduce_sum(class_true * class_pred)\n","      union = tf.reduce_sum(class_true) + tf.reduce_sum(class_pred) - intersection\n","    \n","      iou = (intersection + 1e-7) / (union + 1e-7)\n","      per_class_iou.append(iou)\n","\n","    return tf.reduce_mean(per_class_iou)\n","\n","# A metric to check if plants and weeds are interpreted as background\n","def bg_over_something(y_true, y_pred):\n","    # get predicted class from softmax\n","    y_pred = tf.expand_dims(tf.argmax(y_pred, -1), -1)\n","\n","    class_true_plant = tf.cast(tf.where(y_true == 1, 1, 0), tf.float32)\n","    class_true_weed = tf.cast(tf.where(y_true == 2, 1, 0), tf.float32)\n","    class_pred_bg = tf.cast(tf.where(y_pred == 0, 1, 0), tf.float32)\n","\n","    class_true_no_bg = class_true_plant + class_true_weed\n","\n","    intersection = tf.reduce_sum(class_true_no_bg * class_pred_bg)\n","\n","    return (intersection + 1e-7) / (tf.reduce_sum(class_true_no_bg) + 1e-7)\n","\n","\n","# Validation metrics\n","# ------------------\n","metrics = ['accuracy', meanIoU, bg_over_something]\n","# ------------------\n","\n","# Compile Model\n","model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T8wqoQqQfIN7"},"source":["#Create callbacks and train the model\n","import  os\n","from datetime import datetime\n","\n","cwd = os.getcwd()\n","\n","exps_dir = os.path.join(cwd, 'drive/My Drive/Homeworks/Homework2/Saved', 'multiclass_segmentation_experiments')\n","if not os.path.exists(exps_dir):\n","    os.makedirs(exps_dir)\n","\n","now = datetime.now().strftime('%b%d_%H-%M-%S')\n","\n","model_name = 'CNN_UNet_Bipbip_Haricot'\n","\n","exp_dir = os.path.join(exps_dir, model_name + '_' + str(now))\n","if not os.path.exists(exp_dir):\n","    os.makedirs(exp_dir)\n","    \n","callbacks = []\n","\n","# Model checkpoint\n","# ----------------\n","ckpt_dir = os.path.join(exp_dir, 'ckpts')\n","if not os.path.exists(ckpt_dir):\n","    os.makedirs(ckpt_dir)\n","\n","ckpt_callback = tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(ckpt_dir, 'cp_{epoch:02d}.ckpt'), \n","                                                   save_weights_only=True)  # False to save the model directly\n","callbacks.append(ckpt_callback)\n","\n","# Visualize Learning on Tensorboard\n","# ---------------------------------\n","tb_dir = os.path.join(exp_dir, 'tb_logs')\n","if not os.path.exists(tb_dir):\n","    os.makedirs(tb_dir)\n","    \n","# By default shows losses and metrics for both training and validation\n","tb_callback = tf.keras.callbacks.TensorBoard(log_dir=tb_dir,\n","                                             profile_batch=0,\n","                                             histogram_freq=0)  # if 1 shows weights histograms\n","callbacks.append(tb_callback)\n","\n","# Early Stopping\n","# --------------\n","early_stop = False\n","if early_stop:\n","    es_callback = tf.keras.callback.EarlyStopping(monitor='val_loss', patience=10)\n","    callbacks.append(es_callback)\n","\n","\n","model.fit(x=train_dataset,\n","          epochs=100,  #### set repeat in training dataset\n","          steps_per_epoch=len(dataset)//batch_size,\n","          validation_data=valid_dataset,\n","          validation_steps=len(dataset_valid)//batch_size, \n","          callbacks=callbacks)\n","\n","# How to visualize Tensorboard\n","\n","# 1. tensorboard --logdir EXPERIMENTS_DIR --port PORT     <- from terminal\n","# 2. localhost:PORT   <- in your browser"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zim3I4cEy3jr"},"source":["#Load weights\n","\n","load_weights = False\n","\n","loaded_weights = 'CNN_UNet_Bipbip_Haricot_Dec10_16-41-19/ckpts/cp_78.ckpt'\n","\n","if load_weights:\n","  model.load_weights('/content/drive/My Drive/Homeworks/Homework2/Saved/multiclass_segmentation_experiments/' + str(loaded_weights))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Gbe2txuZ4bN-"},"source":["#Functions from starting_kit to encode/decode predicted mask\n","import os\n","import json\n","import numpy as np\n","from PIL import Image\n","from datetime import datetime\n","\n","\n","def rle_decode(rle, shape):\n","    s = rle.split()\n","    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n","    starts -= 1\n","    ends = starts + lengths\n","    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n","    for lo, hi in zip(starts, ends):\n","        img[lo:hi] = 1\n","    return img.reshape(shape)\n","\n","\n","def rle_encode(img):\n","    '''\n","    img: numpy array, 1 - foreground, 0 - background\n","    Returns run length as string formatted\n","    '''\n","    pixels = img.flatten()\n","    pixels = np.concatenate([[0], pixels, [0]])\n","    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n","    runs[1::2] -= runs[::2]\n","\n","    return ' '.join(str(x) for x in runs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aCrdm-0p1s9n","executionInfo":{"status":"ok","timestamp":1607616896288,"user_tz":-60,"elapsed":473,"user":{"displayName":"Fraq Masq","photoUrl":"","userId":"17836806757551179114"}},"outputId":"065eccd1-726d-412e-c524-78d49c7eb0aa"},"source":["#Testing start here->create test generator\n","from PIL import Image\n","\n","test_dataset_dir = os.path.join(cwd, 'Groupped_Images/Test_Dev')\n","\n","\n","test_data_gen = ImageDataGenerator()\n","test_gen = test_data_gen.flow_from_directory(test_dataset_dir,\n","                                             color_mode='rgb',\n","                                             classes=['Images'],\n","                                             # don't generate labels\n","                                             class_mode=None,\n","                                             # don't shuffle\n","                                             shuffle=False,\n","                                             # use same size as in training\n","                                             target_size=(img_h, img_w),\n","                                             batch_size=1)\n","\n","\n","test_gen.reset()\n","img_names = []\n","img_shapes = []\n","for file in test_gen.filenames:\n","  img_names.append(file.strip('.jpg')[7:])\n","  img_shapes.append(Image.open(os.path.join(test_dataset_dir, file)).size)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Found 120 images belonging to 1 classes.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"H9aOcu_sOXIs"},"source":["#Script that create the submitted json, predicting a mask for each Test_Dev image\n","fname = 'results_'\n","fname += datetime.now().strftime('%b%d_%H-%M-%S') + '.json'\n","result_dir = os.path.join(cwd, 'drive/My Drive/Homeworks/Homework2')\n","submission_dict = {}\n","\n","\n","for img, img_name, shape  in zip(test_gen, img_names, img_shapes):\n","  test_img_h = shape[1]\n","  test_img_w = shape[0]\n","  img = img[0]\n","  team = ''\n","  crop = ''\n","\n","  if 'Bipbip' in img_name:\n","    team = 'Bipbip'\n","  elif 'Pead' in img_name:\n","    team = 'Pead'\n","  elif 'Roseau' in img_name:\n","    team = 'Roseau'\n","  elif 'Weedelec' in img_name:\n","    team = 'Weedelec'\n","  \n","  if 'haricot' in img_name:\n","    crop = 'Haricot'\n","  elif 'mais' in img_name:\n","    crop = 'Mais'\n","  \n","\n","  out_sigmoid = model.predict(x=tf.expand_dims(img, 0))\n","  \n","  out_sigmoid = tf.image.resize(out_sigmoid, [test_img_h,test_img_w], method='nearest')\n","  \n","  predicted_class = tf.argmax(out_sigmoid, -1)\n","\n","  predicted_class = predicted_class[0, ...]\n","\n","\n","  submission_dict[img_name] = {}\n","  submission_dict[img_name]['shape'] = predicted_class.numpy().shape #[test_img_h,test_img_w]\n","  submission_dict[img_name]['team'] = team\n","  submission_dict[img_name]['crop'] = crop\n","  submission_dict[img_name]['segmentation'] = {}\n","\n","\n","\n","  # RLE encoding\n","  # crop\n","  rle_encoded_crop = rle_encode(predicted_class.numpy() == 1)\n","  # weed\n","  rle_encoded_weed = rle_encode(predicted_class.numpy() == 2)\n","\n","\n","  submission_dict[img_name]['segmentation']['crop'] = rle_encoded_crop\n","  submission_dict[img_name]['segmentation']['weed'] = rle_encoded_weed\n","\n","# Finally, save the results into the submission.json file\n","with open(os.path.join(result_dir, fname), 'w') as f:\n","  json.dump(submission_dict, f)\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DYFwNQeJ6Tk6"},"source":["\n","#Just for visualization -> retrieve the last encoded mask and plot it with the original one\n","import time\n","from matplotlib import cm\n","import matplotlib.pyplot as plt\n","\n","# Assign a color to each class\n","colors_dict = {}\n","colors_dict[2] = [255, 0, 0]  # weed\n","colors_dict[1] = [255, 255, 255]  # crop\n","colors_dict[0] = [0, 0, 0]  # background \n","\n","\n","img_shape = shape[1], shape[0]\n","fig, ax = plt.subplots(1, 3, figsize=(8, 8))\n","fig.show()\n","\n","\n","# Reconstruct crop and weed binary masks\n","crop_mask = rle_decode(rle_encoded_crop, shape=img_shape)\n","weed_mask = rle_decode(rle_encoded_weed, shape=img_shape)\n","\n","# Reconstruct original mask\n","# weed_mask * 2 allows to convert ones into target 2 (weed label)\n","reconstructed_mask = crop_mask + (weed_mask * 2)\n","np.testing.assert_allclose(predicted_class , reconstructed_mask)\n","reconstructed_rgb_arr = np.zeros(shape=[test_img_h, test_img_w, 3])\n","reconstructed_rgb_arr[np.where(reconstructed_mask == 1)] = colors_dict[1]\n","reconstructed_rgb_arr[np.where(reconstructed_mask == 2)] = colors_dict[2]\n","#np.testing.assert_allclose(prediction_img, reconstructed_rgb_arr)\n","\n","#vis_prediction_img = np.zeros([test_img_h,test_img_w, 3])\n","prediction_img = np.zeros([test_img_h,test_img_w, 3])\n","prediction_img[np.where(predicted_class == 0)] = colors_dict[0]\n","for i in range(1, 3):\n","  prediction_img[np.where(predicted_class == i)] = colors_dict[i]\n","\n","\n","ax[0].imshow(np.uint8(img))\n","ax[1].imshow(np.uint8(prediction_img))\n","ax[2].imshow(np.uint8(reconstructed_rgb_arr))\n","\n","fig.canvas.draw()\n","time.sleep(1)\n"],"execution_count":null,"outputs":[]}]}